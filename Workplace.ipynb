{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workplace.ipynb",
      "provenance": [],
      "mount_file_id": "1Y4c_Oyq3crtsl3BmwfP2XelPRxc13DsK",
      "authorship_tag": "ABX9TyNca9+4t2kXrUhzj8/k5NVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrii-Radyhin/Andrii-Radyhin/blob/main/Workplace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset -> https://www.kaggle.com/datasets/rajkumarl/- people-clothing-segmentation\n",
        "- Architecture -> EfficientnetB3 (320,300)\n",
        "- loss -> BCE\n",
        "- optimizer -> Adam\n",
        "- Required OpenCV catalyst Pytorch numpy matplotlib pandas"
      ],
      "metadata": {
        "id": "EFtrjvW3FYQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Config"
      ],
      "metadata": {
        "id": "27m3pQaUTxQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (320, 300) \n",
        "crop={'x_min': 115,\n",
        "      'y_min': 210,\n",
        "      'x_max': 350,\n",
        "      'y_max':445}"
      ],
      "metadata": {
        "id": "9_YcaMOaTw4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BaJU6M47AGCm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Opens the inbuilt camera of laptop to capture video.\n",
        "directory='/content/framess2'\n",
        "path='/content/drive/MyDrive/Camera 3_20220526_003249.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(path)\n",
        "i = 1\n",
        "\n",
        "os.chdir(directory)\n",
        "\n",
        "j=0\n",
        "while(cap.isOpened()):\n",
        "    ret, frame = cap.read()\n",
        "     \n",
        "    # This condition prevents from infinite looping\n",
        "    # incase video ends.\n",
        "    if ret == False:\n",
        "        break\n",
        "     \n",
        "    # Save Frame by Frame into disk using imwrite method\n",
        "    if (i % 1000) == 0:\n",
        "      cv2.imwrite(str(j)+'.jpg', frame)\n",
        "      j+=1\n",
        "    i += 1\n",
        " \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "ck7agjhMA6rQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opencv+numpy "
      ],
      "metadata": {
        "id": "rn4Divd1DwXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "\n",
        "images=np.array()\n",
        "\n",
        "folder_dir = \"/kaggle/png_images\"\n",
        "\n",
        "for img in os.listdir(folder_dir):\n",
        "  img = cv2.imread(img)\n",
        "\n",
        "  crop_img = img[crop['y_min'] : crop['y_max'],\n",
        "                 crop['x_min'] : crop['x_max']]\n",
        "\n",
        "  #normalize in range[0,1]\n",
        "  crop_img = (crop_img-min(crop_img))/(max(crop_img)-min(crop_img)) \n",
        "  images.append(crop_img)\n",
        "\n",
        "images = ToTensor(images)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "images = normalize(images)"
      ],
      "metadata": {
        "id": "vJ6fd1lhCHeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, target, test_size=0.33, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "0k5DwpATEkG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "5wzVJ3NcFptd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = TensorDataset(X_train, y_train)\n",
        "dataset_test = TensorDataset(X_test, y_test)\n",
        "loaders = {\n",
        "    \"train\": DataLoader(dataset_train, batch_size=32, num_workers=1),\n",
        "    \"valid\": DataLoader(dataset_test, batch_size=32, num_workers=1)\n",
        "    }"
      ],
      "metadata": {
        "id": "zr8vD7cUFIBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "VjdaQ25BW-wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "\n",
        "model = models.efficientnet_b3(pretrained=True)\n",
        "model.train()\n",
        "model.classifier[1]=torch.nn.Linear(in_features=1536, out_features=1, bias=True)\n",
        "\n",
        "model=model.to(device)"
      ],
      "metadata": {
        "id": "H9_A-gv3Ranl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-3)"
      ],
      "metadata": {
        "id": "hTtJpf0-XmP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "\n",
        "for epoch in range(5000):\n",
        "    order = np.random.permutation(len(X_train))\n",
        "    for start_index in range(0, len(X_train), batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        batch_indexes = order[start_index:start_index+batch_size]\n",
        "        \n",
        "        x_batch = X_train[batch_indexes]\n",
        "        y_batch = y_train[batch_indexes]\n",
        "        \n",
        "        preds = model(x_batch)['out']\n",
        "        \n",
        "        loss_value = loss(preds, y_batch)\n",
        "        loss_value.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "    if epoch % 100 == 0:\n",
        "        test_preds = model(X_test)['out']\n",
        "        test_preds = test_preds.argmax(dim=1)\n",
        "        print((test_preds == y_test).float().mean())"
      ],
      "metadata": {
        "id": "pyJUZNUuY5PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "15/5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frOu9DScCY3U",
        "outputId": "b51fb871-2cc2-4765-f770-5da9cf9c20fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}